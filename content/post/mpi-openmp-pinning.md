---
title: MPIとOpenMPにおけるプロセッサアフィニティの設定
date: 2020-09-28T11:40:41+09:00
description:
tags: []
draft: true
---

プロセッサアフィニティとは，OSにおいてタスク (プロセスあるいはスレッド) が実行される
プロセッサを限定することを指します．通常，OSのスケジューラはプロセッサ間の
負荷分散のため，タスクを実行するプロセッサを切り替えていきます．
しかし，タスクが実行されるプロセッサが変更されると，キャッシュが無効化されたり，
NUMAノードを跨ぐメモリアクセスが増加し，アプリケーションの性能低下を招く場合があります．
プロセッサアフィニティを設定することにより，これらの問題を避けることができます．

HPCでは性能を最大化するためプロセッサアフィニティを設定することが多いの
ですが，毎回方法を忘れてしまうので，メモを残しておくことにしました．
なお，ここではMPI/OpenMPアプリケーションのみ対象としています．

## MPI

Open MPI: `--bind-to`, `--map-by`

Intel MPIなど各ベンダのMPIは未調査

ググると`taskset`や`numactl`を使用する方法が出てきますが，MPIは使用しなく良いはず．

## OpenMP

`OMP_PLACES`, `OMP_PROC_BIND`

Intel固有の`KMP_AFFINITY`環境変数や，GCC固有の`GOMP_CPU_AFFINTY`環境変数

OpenMP 4.0

`lstopo`

## アフィニティの確認

複雑なアフィニティ設定になると，正しくアフィニティを設定できたのか確認したいこ
とがあります．ググるとアフィニティを確認するツールが色々出てくるのですが，
個人的にはTACCが開発している[amask](https://github.com/TACC/amask)というツールが
ちゃんとメンテされており，使いやすいと思いました．

```
git clone https://github.com/tacc/amask
cd amask
```

`bin/amask_mpi`
`bin/amask_omp`
`bin/amask_hybrid`


出力の意味は書かれているとおりですが，
各行がハードウェアスレッドに対応します．

ランク1のスレッド2なら，6+10=16

```
$ mpirun -n 2 --bind-to socket --map-by socket -x OMP_NUM_THREADS=14 -x OMP_PLACES=cores -x OMP_PROC_BIND=spread ./amask_hybrid

     Each row of matrix is a mask for a Hardware Thread (hwt).
     CORE ID  = matrix digit + column group # in |...|
     A set mask bit (proc-id) = core id + add 28 to each additional row.

rank |    0    |   10    |   20    |
0000 0===========================
     0---------------------------
0001 ==============4=============
     --------------4-------------

          Each row of matrix is a mask for a Hardware Thread (hwt).
          CORE ID  = matrix digit + column group # in |...|
          A set mask bit (proc-id) = core id + add 28 to each additional row.

rank thrd |    0    |   10    |   20    |
0000 0000 0===========================
          0---------------------------
0000 0001 =1==========================
          -1--------------------------
0000 0002 ==2=========================
          --2-------------------------
0000 0003 ===3========================
          ---3------------------------
0000 0004 ====4=======================
          ----4-----------------------
0000 0005 =====5======================
          -----5----------------------
0000 0006 ======6=====================
          ------6---------------------
0000 0007 =======7====================
          -------7--------------------
0000 0008 ========8===================
          --------8-------------------
0000 0009 =========9==================
          ---------9------------------
0000 0010 ==========0=================
          ----------0-----------------
0000 0011 ===========1================
          -----------1----------------
0000 0012 ============2===============
          ------------2---------------
0000 0013 =============3==============
          -------------3--------------
0001 0000 ==============4=============
          --------------4-------------
0001 0001 ===============5============
          ---------------5------------
0001 0002 ================6===========
          ----------------6-----------
0001 0003 =================7==========
          -----------------7----------
0001 0004 ==================8=========
          ------------------8---------
0001 0005 ===================9========
          -------------------9--------
0001 0006 ====================0=======
          --------------------0-------
0001 0007 =====================1======
          ---------------------1------
0001 0008 ======================2=====
          ----------------------2-----
0001 0009 =======================3====
          -----------------------3----
0001 0010 ========================4===
          ------------------------4---
0001 0011 =========================5==
          -------------------------5--
0001 0012 ==========================6=
          --------------------------6-
0001 0013 ===========================7
          ---------------------------7
```

`mpirun --bind-to socket --report-bindings hostname`

`I_MPI_DEBUG=4`
